{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peak images: 20\n",
      "Number of water images: 20\n",
      "Number of label images: 20\n",
      "<class 'numpy.ndarray'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pkg import c, m, f \n",
    "import torch\n",
    "import logging\n",
    "from pkg import c, m, f \n",
    "\n",
    "# classes \n",
    "paths = c.PathManager()\n",
    "data = c.PeakImageDataset(paths)\n",
    "prep = c.DataPreparation(paths, data, batch_size=1)\n",
    "water_h5 = data.load_h5(paths.water_background_h5)\n",
    "ip = c.ImageProcessor(water_h5)\n",
    "p = c.PeakThresholdProcessor(threshold_value=10)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BasicCNN1', 'BasicCNN2', 'DenseNet121_Weights', 'DenseNetBraggPeakClassifier', 'F', 'ResNet50BraggPeakClassifier', 'ResNet50_Weights', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'models', 'nn', 'np', 'optim', 'os', 'torch']\n"
     ]
    }
   ],
   "source": [
    "# if not already generated \n",
    "# ip.process_directory(paths, p.threshold_value) \n",
    "print(dir(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion:  BCEWithLogitsLoss()\n",
      "Optimizer: \n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Learning rate:  0.001\n",
      "Data prepared.\n",
      "Train size: 16\n",
      "Test size: 4\n",
      "Batch size: 1\n",
      "Number of batches in train_loader: 16 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = m.ResNet50BraggPeakClassifier()\n",
    "model = m.ResNet50BraggPeakClassifier().to(device)\n",
    "# loss function/ combines a Sigmoid layer and the BCELoss in one single class\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Criterion: \", criterion)\n",
    "print(\"Optimizer: \\n\", optimizer)\n",
    "print(\"Learning rate: \", optimizer.param_groups[0]['lr'])\n",
    "# data loaders \n",
    "train_loader, test_loader = prep.prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 22:07:32,603 - INFO - Staring training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# backward pass/optimize\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Training the model\"\"\"\n",
    "num_epochs = 10\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# logging.info('Staring training...')\n",
    "print('Staring training...')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader: \n",
    "        peak_images, water_images = inputs\n",
    "        # Move data to GPU\n",
    "        peak_images = peak_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(peak_images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward pass/optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # logging.info(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, train_loader, test_loader, num_epochs, device, criterion, optimizer):\n",
    "    print(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    train_losses = test_losses = []\n",
    "\n",
    "    print('Staring training...')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for inputs, labels in train_loader: \n",
    "            peak_images, water_images = inputs[0].to(device), inputs[1].to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # zero parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(peak_images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward pass/optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            # calculate accuracy\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct_predictions += (predicted == labels).float().sum()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct_predictions / total_predictions\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}, Accuracy: {epoch_acc}')\n",
    "        \n",
    "        print('Starting testing...')\n",
    "        \n",
    "    model.eval()  \n",
    "    test_loss = correct_peaks = total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            peak_images, water_images = inputs[0].to(device), inputs[1].to(device)\n",
    "            # forward pass prediction\n",
    "            peak_pred = model(peak_images) \n",
    "            # compute loss\n",
    "            loss = criterion(peak_pred, labels)\n",
    "            test_loss += loss.item()\n",
    "            # convert predictions to binary to compare with labels \n",
    "            peak_prediction = torch.sigmoid(peak_pred) > 0.5 \n",
    "            # calculate accuracy of peak detection\n",
    "            correct_peaks += (peak_prediction == labels).sum().item()\n",
    "            total += np.prod(labels.shape) \n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        peak_detection_accuracy = correct_peaks / total\n",
    "        test_losses.append(avg_test_loss)\n",
    "        # log results \n",
    "        print(f\"Test Loss: {avg_test_loss:.4f}, Bragg Peak Detection Accuracy: {peak_detection_accuracy:.4f}\")\n",
    "        print(f'Test loss: {test_loss}')\n",
    "        \n",
    "    print('Testing completed.')\n",
    "    \n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(test_losses, color='r')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Losses')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
