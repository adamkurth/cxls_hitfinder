{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pkg import c, m, f \n",
    "import torch\n",
    "import logging\n",
    "from pkg import c, m, f \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peak images: 300\n",
      "Number of water images: 300\n",
      "Number of label images: 300\n",
      "<class 'numpy.ndarray'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "cfg = {\n",
    "    'batch_size': 50,\n",
    "    'threshold_value': 10,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'classes': 2\n",
    "}\n",
    "\n",
    "# classes \n",
    "paths = c.PathManager()\n",
    "data = c.PeakImageDataset(paths)\n",
    "prep = c.DataPreparation(paths, data, batch_size=cfg['batch_size'])\n",
    "water_h5 = data.load_h5(paths.water_background_h5)\n",
    "ip = c.ImageProcessor(water_h5)\n",
    "p = c.PeakThresholdProcessor(threshold_value=cfg['threshold_value'])\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BasicCNN1', 'BasicCNN2', 'DenseNet121_Weights', 'DenseNetBraggPeakClassifier', 'F', 'ResNet50BraggPeakClassifier', 'ResNet50_Weights', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'models', 'nn', 'np', 'optim', 'os', 'torch']\n"
     ]
    }
   ],
   "source": [
    "# if not already generated \n",
    "# ip.process_directory(paths, p.threshold_value) \n",
    "print(dir(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " myModel has 23,503,809 parameter(s) to trained\n",
      " myModel has 0 parameter(s) to trained\n"
     ]
    }
   ],
   "source": [
    "# models = [\n",
    "#     m.BasicCNN1(),\n",
    "#     m.BasicCNN2(),\n",
    "#     m.ResNet50BraggPeakClassifier(),\n",
    "#     m.DenseNetBraggPeakClassifier()\n",
    "# ]\n",
    "\n",
    "models = m.ResNet50BraggPeakClassifier()\n",
    "\n",
    "nbr_param = sum(p.numel() for p in models.parameters() if p.requires_grad)\n",
    "print(\" myModel has {:,d} parameter(s) to trained\".format(nbr_param))\n",
    "\n",
    "for param in models.parameters():\n",
    "    param.requires_grad = False \n",
    "# count the number of parameter to trained\n",
    "nbr_param = sum(p.numel() for p in models.parameters() if p.requires_grad)\n",
    "print(\" myModel has {:,d} parameter(s) to trained\".format(nbr_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion:  BCEWithLogitsLoss()\n",
      "Optimizer:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Learning rate:  0.001\n",
      "Data prepared.\n",
      "Train size: 240\n",
      "Test size: 60\n",
      "Batch size: 50\n",
      "Number of batches in train_loader: 5 \n",
      "\n",
      "\n",
      "Model: ResNet50BraggPeakClassifier\n",
      "Training and testing the model...\n",
      "-- epoch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(loader[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(loader[\u001b[38;5;241m1\u001b[39m])]\n\u001b[0;32m     17\u001b[0m classes \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m f\u001b[38;5;241m.\u001b[39mtrain_test_model(model, loader, criterion, optimizer, epochs, device, N, batch, classes)\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mcpu()  \u001b[38;5;66;03m# Move model back to CPU to free up GPU memory\u001b[39;00m\n\u001b[0;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32mc:\\Users\\eseveret\\Desktop\\cxls_hitfinder\\cnn\\src\\pkg\\functions.py:165\u001b[0m, in \u001b[0;36mtrain_test_model\u001b[1;34m(model, loader, criterion, optimizer, epochs, device, N, batch, classes)\u001b[0m\n\u001b[0;32m    162\u001b[0m score \u001b[38;5;241m=\u001b[39m model(peak_images)\n\u001b[0;32m    163\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(score, labels)\n\u001b[1;32m--> 165\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    166\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    167\u001b[0m running_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Convert to Python number with .item()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eseveret\\AppData\\Local\\anaconda3\\envs\\hitfinder_env\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eseveret\\AppData\\Local\\anaconda3\\envs\\hitfinder_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "i = models\n",
    "model = i.to(device)\n",
    "# loss function/ combines a Sigmoid layer and the BCELoss in one single class\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg['learning_rate'])\n",
    "print(\"Criterion: \", criterion)\n",
    "print(\"Optimizer: \", optimizer)\n",
    "print(\"Learning rate: \", optimizer.param_groups[0]['lr'])\n",
    "# data loaders \n",
    "train_loader, test_loader = prep.prep_data()\n",
    "loader = [train_loader, test_loader]\n",
    "N = [len(loader[0].dataset), len(loader[1].dataset)]\n",
    "\n",
    "epochs = cfg['num_epochs']\n",
    "batch = [len(loader[0]), len(loader[1])]\n",
    "classes = cfg['classes']\n",
    "\n",
    "f.train_test_model(model, loader, criterion, optimizer, epochs, device, N, batch, classes)\n",
    "\n",
    "model.cpu()  # Move model back to CPU to free up GPU memory\n",
    "torch.cuda.empty_cache()  # Clear unused memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
